from art.attacks.evasion import FastGradientMethod
from art.estimators.classification import KerasClassifier
from keras.models import load_model
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf

# Disable eager execution to accommodate ART
tf.compat.v1.disable_eager_execution()

# Load your pre-trained model and create an ART classifier
model = load_model(r"D:\Repos\mw-detection-using-image\trained_model.h5")
classifier = KerasClassifier(model=model, clip_values=(0, 1), use_logits=False)

# Step 2: Choose an attack method (e.g., Fast Gradient Method)
attack = FastGradientMethod(estimator=classifier, eps=0.2)

# Step 3: Configure the attack to target the backdoor
# Define parameters such as epsilon and the target label
# For example, if the backdoor activates with label 9:
target_label = 9
attack_params = {"eps": 0.2, "targeted": True, "target": np.array([target_label])}

# inputs
train_img_list = np.load(r"D:\Repos\mw-detection-using-image\img_list.npy")
labels = np.load(r"D:\Repos\mw-detection-using-image\labels.npy")
x_train, x_test, y_train, y_test = train_test_split(
    np.asarray(train_img_list), np.asarray(labels), test_size=0.3, random_state=25
)

# Step 4: Generate adversarial examples
adversarial_examples = attack.generate(x=x_test, **attack_params)

# Step 5: Evaluate Model Behavior
adversarial_predictions = classifier.predict(adversarial_examples)
benign_predictions = classifier.predict(x_test)


# Step 6: Analyze Results
def analyze_results(adversarial_preds, benign_preds, threshold=0.5):
    # Convert softmax outputs to binary predictions based on the threshold
    adversarial_preds_binary = (adversarial_preds > threshold).astype(int)
    benign_preds_binary = (benign_preds > threshold).astype(int)

    # Check if the model produced unintended outputs or behaved differently
    deviations = np.sum(benign_preds_binary != adversarial_preds_binary)

    # Determine if the backdoor is triggered by checking the target label
    backdoor_triggered = adversarial_preds_binary[:, target_label] == 1

    return backdoor_triggered, deviations


# Evaluate and analyze results
backdoor_triggered, deviations = analyze_results(
    adversarial_predictions, benign_predictions
)

# Print results
print(
    f"Backdoor Triggered: {backdoor_triggered.sum()} out of {len(adversarial_examples)} examples"
)
print(
    f"Deviations from Expected Behavior: {deviations} out of {len(adversarial_examples)} examples"
)

# Step 7: Iterate and Refine (if needed)
if backdoor_triggered.any():
    print("Backdoor detected. Consider refining attack parameters or strategies.")
    # You can modify attack parameters and regenerate adversarial examples
    # Re-run steps 5 and 6 to evaluate the refined adversarial examples
else:
    print("No backdoor detected. Continue monitoring and refining if necessary.")
