from art.attacks.poisoning import FeatureCollisionAttack
from art.estimators.classification import KerasClassifier
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model

tf.compat.v1.disable_eager_execution()


# Load your trained CNN model
model = load_model(r"D:\Repos\mw-detection-using-image\trained_model.h5")

# Prepare the backdoor trigger
# The trigger needs to be the same shape as the model input, e.g., a small white square in the corner
trigger = np.zeros((128, 128, 1), dtype=np.float32)
trigger[
    -10:, -10:, :
] = 1  # Example trigger: a 10x10 white square in the bottom-right corner

# Prepare the target label for the backdoor attack
target_label = np.array(
    [1]
)  # This is an example; use the target label you want to trigger

# Wrap the model with ART's KerasClassifier
classifier = KerasClassifier(model=model, clip_values=(0, 1))

# Initialize the backdoor attack object
attack = FeatureCollisionAttack(
    classifier=classifier, target=target_label, feature_layer=11
)

# Assume we have a batch of images and labels to poison
images_to_poison = np.load(r"D:\Repos\mw-detection-using-image\img_list.npy")
labels_to_poison = np.load(r"D:\Repos\mw-detection-using-image\labels.npy")

# Generate the poisoned samples and labels
poisoned_images, poisoned_labels = attack.poison(images_to_poison, y=labels_to_poison)

# Now you can use the poisoned_images and poisoned_labels as needed
# For example, you can save them to disk
np.save(r"D:\Repos\mw-detection-using-image\poisoned_images.npy", poisoned_images)
np.save(r"D:\Repos\mw-detection-using-image\poisoned_labels.npy", poisoned_labels)

# You can also evaluate the ART classifier on the poisoned samples
predictions = classifier.predict(poisoned_images)
accuracy = np.sum(np.argmax(predictions, axis=1) == poisoned_labels) / len(
    poisoned_labels
)
print("Accuracy on adversarial samples: {}%".format(accuracy * 100))

# You can also evaluate the ART classifier on the clean samples
predictions = classifier.predict(images_to_poison)
accuracy = np.sum(np.argmax(predictions, axis=1) == labels_to_poison) / len(
    labels_to_poison
)
print("Accuracy on clean samples: {}%".format(accuracy * 100))
