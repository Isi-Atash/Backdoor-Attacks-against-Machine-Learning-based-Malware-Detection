import os
import numpy as np
from keras.models import Sequential
from keras.layers import (
    Dense,
    Conv2D,
    Flatten,
    MaxPooling2D,
    BatchNormalization,
    Dropout,
)
from keras.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
from summary import summarize_diagnostics, plot_dataset, visualize_model

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# Choosing the size of input images and proportion of dataset split.
h, w = 128, 128
test_size = 0.15
train_size = 0.6
validation_size = 0.25

try:
    # loading prepared samples.
    train_img_list = np.load(
        r"D:\Repos\mw-detection-using-image\data\npy\img_list_original.npy"
    )
    labels = np.load(r"D:\Repos\mw-detection-using-image\data\npy\labels_original.npy")

    # First split the data into training + validation and test sets
    train_val_x, test_x, train_val_y, test_y = train_test_split(
        train_img_list, labels, test_size=test_size, random_state=25
    )

    # Then split the training + validation set into separate training and validation sets
    train_size_adjusted = train_size / (
        1 - test_size
    )  # Adjust train size for the remaining dataset
    train_x, val_x, train_y, val_y = train_test_split(
        train_val_x, train_val_y, test_size=1 - train_size_adjusted, random_state=25
    )

except FileNotFoundError as e:
    print(f"Error: {e.strerror}. File {e.filename} not found.")
    # Handle the error by possibly exiting the script, or providing an alternative data source
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    # Handle any other exception that might occur


print(f"No. of training samples: {train_x.shape[0]}")
print(
    f"No. of validation samples: {val_x.shape[0]}"
)  # Assuming val_x is your validation set
print(f"No. of training labels: {train_y.shape[0]}")
print(
    f"No. of validation labels: {val_y.shape[0]}"
)  # Assuming val_y is your validation set labels
print(f"No. of testing samples: {test_x.shape[0]}")
print(f"No. of testing labels: {test_y.shape[0]}")


def create_cnn_model(input_shape):
    model = Sequential(
        [
            # First block of convolutions and pooling
            Conv2D(
                32, (3, 3), padding="same", input_shape=input_shape, activation="relu"
            ),
            Conv2D(32, (3, 3), activation="relu"),
            MaxPooling2D(pool_size=(2, 2)),
            BatchNormalization(),
            # Second block of convolutions and pooling
            Conv2D(64, (3, 3), padding="same", activation="relu"),
            Conv2D(64, (3, 3), activation="relu"),
            MaxPooling2D(pool_size=(2, 2)),
            BatchNormalization(),
            Dropout(0.2),
            # Flattening and Dense layers
            Flatten(),
            Dense(100, activation="relu"),
            Dense(10, activation="relu"),
            Dense(1, activation="sigmoid"),
        ]
    )

    # Compiling the model.
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

    return model


def train_model(model, train_x, train_y, validation_data, epochs, callbacks=None):
    history = model.fit(
        train_x,
        train_y,
        epochs=epochs,
        validation_data=validation_data,
        shuffle=True,
        callbacks=callbacks,
    )
    return history


# Define and compile the CNN model
input_shape = (h, w, 1)
model = create_cnn_model(input_shape)
visualize_model(model)
model.summary()

# Define ModelCheckpoint to save the best model
checkpoint = ModelCheckpoint(
    r"D:\Repos\mw-detection-using-image\models\best_model_original.h5",
    monitor="val_loss",
    save_best_only=True,
)


# Train the model
# epochs = 15
epochs = 1
validation_data = (val_x, val_y)
history = train_model(model, train_x, train_y, validation_data, epochs, [checkpoint])

# testing the model with test dataset.
_, acc = model.evaluate(test_x, test_y)

# Display training history and save plots
summarize_diagnostics(history)
plot_dataset([train_size, validation_size, test_size])
print(f"Testing accuracy is {acc * 100}%")
