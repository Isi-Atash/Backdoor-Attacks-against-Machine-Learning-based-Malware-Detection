import os
import numpy as np
from keras.callbacks import ModelCheckpoint
import summary as sm
import common_methods as cm
from keras.callbacks import EarlyStopping
from keras.callbacks import LearningRateScheduler


os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# Directories
# Define file paths
img_list_path = r"D:\Repos\mw-detection-using-image\data\npy\img_list_poisoned_1.npy"
labels_path = r"D:\Repos\mw-detection-using-image\data\npy\labels_poisoned_1.npy"

# Model save location
model_save_path = r"D:\Repos\mw-detection-using-image\models\best_model_poisoned_1.h5"

# Define the file path where you want to save the results
results_file_path = r"D:\Repos\mw-detection-using-image\doc\model_results\poisoned"

# Summary directories
# Define the base directory where you want to save the summary
base_dir = r"D:\Repos\mw-detection-using-image\images\poisoned"

# Load data
train_img_list, labels = cm.load_data(img_list_path, labels_path)

# Check if data is loaded successfully
if train_img_list is not None and labels is not None:
    # Split data
    h, w = 128, 128
    train_size = 0.6
    validation_size = 0.25
    test_size = 0.15

    train_x, val_x, train_y, val_y, test_x, test_y = cm.split_data(
        train_img_list, labels, train_size, validation_size, test_size
    )
else:
    print("Data loading failed. Check file paths and try again.")


print(f"No. of training samples: {train_x.shape[0]}")
print(
    f"No. of validation samples: {val_x.shape[0]}"
)  # Assuming val_x is your validation set
print(f"No. of training labels: {train_y.shape[0]}")
print(
    f"No. of validation labels: {val_y.shape[0]}"
)  # Assuming val_y is your validation set labels
print(f"No. of testing samples: {test_x.shape[0]}")
print(f"No. of testing labels: {test_y.shape[0]}")


# Define and compile the CNN model
input_shape = (h, w, 1)
history = cm.create_cnn_model(input_shape)
sm.visualize_model(history, base_dir)
history.summary()

# Define ModelCheckpoint to save the best model
checkpoint = ModelCheckpoint(
    model_save_path,
    monitor="val_loss",
    save_best_only=True,
)

# Set up early stopping
early_stopping = EarlyStopping(
    monitor="val_loss",  # Monitor the validation loss
    patience=3,  # Number of epochs with no improvement after which training will be stopped
    verbose=1,  # Will print messages when early stopping is triggered
    restore_best_weights=True,  # Restores model weights from the epoch with the best value of the monitored quantity
)

lr_scheduler = LearningRateScheduler(cm.scheduler, verbose=1)


# Train the model
epochs = 15
validation_data = (val_x, val_y)
model = cm.create_cnn_model(
    input_shape
)  # Ensure you have defined input_shape and model architecture correctly

history = cm.train_model(
    model,
    train_x,
    train_y,
    validation_data,
    epochs,
    [checkpoint],
    early_stopping,
    lr_scheduler=lr_scheduler,
)

# Testing the model with the test dataset
_, acc = model.evaluate(test_x, test_y)
print(f"Testing accuracy is {acc * 100}%")

# Assuming sm is a module where your summarize_diagnostics and plot_dataset functions are defined
sm.summarize_diagnostics(history, base_dir)
sm.plot_dataset([train_size, validation_size, test_size], base_dir)

memo = "Original dataset without the early stopping, with 15 epochs"
# Save the results to file
cm.save_results_to_file(history, acc, memo, results_file_path)

print(f"Results saved to {results_file_path}")
